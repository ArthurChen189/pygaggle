# Neural Pointwise Ranking Baselines on Robust04 - with TPU

This page contains instructions for running monoT5 on the Robust 04 collection.

Note that this collection uses [TREC Disks 4 & 5](https://trec.nist.gov/data/cd45/index.html), which are only available after you fill and sign a form from NIST. Therefore, only proceed with this documentation if you already have the corpus.

We will focus on using monoT5-3B to rerank, since it is difficult to run such a large model without a TPU.
We also mention the changes required to run monoT5-base for those with a more constrained compute budget.
- monoT5: Document Ranking with a Pretrained Sequence-to-Sequence Model [(Nogueira et al., 2020)](https://www.aclweb.org/anthology/2020.findings-emnlp.63.pdf)

Prior to running this, we suggest looking at our first-stage [BM25 ranking instructions](https://github.com/castorini/anserini/blob/master/docs/experiments-msmarco-passage.md).
We rerank the BM25 run files that contain ~1000 documents per query using monoT5.
monoT5 is a pointwise reranker. This means that each document is scored independently using T5.

Note that we do not train monoT5 on Robust04. Hence the results are **zero-shot**.

## Start a VM with TPU on Google Cloud

Define environment variables.
```
export PROJECT_NAME=<gcloud project name>
export PROJECT_ID=<gcloud project id>
export INSTANCE_NAME=<name of vm to create>
export TPU_NAME=<name of tpu to create>
```

Create the VM.
```
gcloud beta compute --project=${PROJECT_NAME} instances create ${INSTANCE_NAME} --zone=europe-west4-a --machine-type=n1-standard-4 --subnet=default --network-tier=PREMIUM --maintenance-policy=MIGRATE --service-account=${PROJECT_ID}-compute@developer.gserviceaccount.com  --scopes=https://www.googleapis.com/auth/cloud-platform --image=debian-10-buster-v20201112 --image-project=debian-cloud --boot-disk-size=25GB --boot-disk-type=pd-standard --boot-disk-device-name=${INSTANCE_NAME} --reservation-affinity=any
```

It is possible that the `image` and `machine-type` provided here are dated so feel free to update them to whichever fits your needs.
After the VM created, we can `ssh` to the machine.  
Make sure to initialize `PROJECT_NAME` and `TPU_NAME` from within the machine too.
Then create a TPU.

```
curl -O https://dl.google.com/cloud_tpu/ctpu/latest/linux/ctpu && chmod a+x ctpu
./ctpu up --name=${TPU_NAME} --project=${PROJECT_NAME} --zone=europe-west4-a --tpu-size=v3-8 --tpu-only --noconf
```

## Setup environment on VM

Install required tools including [Miniconda](https://docs.conda.io/en/latest/miniconda.html).
```
sudo apt-get update
sudo apt-get install git gcc screen --yes
curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash ./Miniconda3-latest-Linux-x86_64.sh
source ~/.bashrc
```
Then create a Python virtual environment for the experiments and install dependencies.
```
conda init
conda create --y --name py36 python=3.6
conda activate py36
conda install -c conda-forge httptools jsonnet --yes
pip install tensorflow tensorflow-text t5[gcp]
git clone https://github.com/castorini/mesh.git
pip install --editable mesh
```

## Data Prep

Since we will use some scripts form PyGaggle to process data and evaluate results, we first install it from source.
```
git clone --recursive https://github.com/castorini/pygaggle.git
cd pygaggle
pip install .
```

We store all the files in the `data/robust04` directory.
```
export DATA_DIR=data/robust04
mkdir ${DATA_DIR}
```

We download the query, qrels, run and corpus files. 

The run file is generated by following the Anserini's [BM25 ranking instructions](https://github.com/castorini/anserini/blob/master/docs/regressions-robust04.md).

In short, the files are:
- `queries.dev.small.tsv`: 6,980 queries from the MS MARCO dev set.
- `qrels.dev.small.tsv`: 7,437 pairs of query relevant passage ids from the MS MARCO dev set.
- `run.dev.small.tsv`: Approximately 6,980,000 pairs of dev set queries and retrieved passages using Anserini's BM25.
- `collection.tar.gz`: All passages (8,841,823) in the MS MARCO passage corpus. In this tsv file, the first column is the passage id, and the second is the passage text.

Let's start.
```
cd ${DATA_DIR}
wget https://storage.googleapis.com/duobert_git/run.bm25.dev.small.tsv
wget https://github.com/castorini/anserini/blob/master/src/main/resources/topics-and-qrels/qrels.robust04.txt
cd ../../
```

Also extract the TREC disks 4 & 5 here (as already mentioned, this collection are only available at the NIST website):
```
## TODO: add code here.
```

As a sanity check, we can evaluate the first-stage retrieved documents using the `trec_eval` tool:
```

```

The output should be:
```
#####################
MRR @10: 0.18736452221767383
QueriesRanked: 6980
#####################
```

Then, we prepare the query-doc pairs in the monoT5 input format.
```
python create_robust04_tsv_dev_pairs_t5_reranker.py \
      --queries_path=${DATA_DIR}/topics.robust04.txt \
      --run_path=${DATA_DIR}/run.dev.txt \
      --corpus_path=${DATA_DIR}/trec_concat.txt \
      --output_segment_texts_path=${DATA_DIR}/segment_texts.txt \
      --output_segment_query_doc_ids_path=${DATA_DIR}/segment_query_doc_ids.tsv

```
We will get two output files here:
- `segment_texts.txt`: The query-doc pairs for monoT5 input.
- `segment_query_doc_ids.tsv`: The `query_id`s and `doc_id`s that map to the query-doc pairs. We will use this to map query-doc pairs to their corresponding monoT5 output scores.

The files are made available in our [bucket](https://console.cloud.google.com/storage/browser/castorini/monot5/data).

Note that there might be a memory issue if the monoT5 input file is too large for the memory in the instance. We thus split the input file into multiple files.

```
split --suffix-length 3 --numeric-suffixes --lines 500000 ${DATA_DIR}/segment_texts.txt ${DATA_DIR}/segment_texts.txt
```

For `segment_texts.txt`, we will get 5 files after split. i.e. (`segment_texts.txt000` to `segment_texts.txt004`).
Note that it is possible that running reranking might still result in OOM issues in which case reduce the number of lines to smaller than `500000`.

We copy these input files to Google Storage. TPU inference will read data directly from `gs`.
```
export GS_FOLDER=<google storage folder to store input/output data>
gsutil cp ${DATA_DIR}/segment_texts.txt??? ${GS_FOLDER}
```
These files can also be found in our [bucket](https://console.cloud.google.com/storage/browser/castorini/monot5/data/robust04).

## Rerank with monoT5

Let's first define the model type and checkpoint.

```
export MODEL_NAME=<base or 3B>
export MODEL_DIR=gs://castorini/monot5/experiments/${MODEL_NAME}
```

Then run following command to start the process in background and monitor the log
```
for ITER in {000..004}; do
  echo "Running iter: $ITER" >> out.log
  nohup t5_mesh_transformer \
    --tpu="${TPU_NAME}" \
    --gcp_project="${PROJECT_NAME}" \
    --tpu_zone="europe-west4-a" \
    --model_dir="${MODEL_DIR}" \
    --gin_file="gs://t5-data/pretrained_models/${MODEL_NAME}/operative_config.gin" \
    --gin_file="infer.gin" \
    --gin_file="beam_search.gin" \
    --gin_param="utils.tpu_mesh_shape.tpu_topology = '2x2'" \
    --gin_param="infer_checkpoint_step = 1100000" \
    --gin_param="utils.run.sequence_length = {'inputs': 512, 'targets': 2}" \
    --gin_param="Bitransformer.decode.max_decode_length = 2" \
    --gin_param="input_filename = '${GS_FOLDER}/segment_texts.txt${ITER}'" \
    --gin_param="output_filename = '${GS_FOLDER}/t5_scores.txt${ITER}'" \
    --gin_param="utils.run.batch_size=('tokens_per_batch', 65536)" \
    --gin_param="Bitransformer.decode.beam_size = 1" \
    --gin_param="Bitransformer.decode.temperature = 0.0" \
    --gin_param="Unitransformer.sample_autoregressive.sampling_keep_top_k = -1" \
    >> out.log 2>&1
done &

tail -100f out.log_eval
```

Using a TPU v3-8, it takes approximately XXX hours and YYY hours to rerank with monoT5-base and monoT5-3B respectively.

Note that we strongly encourage you to run any of the long processes in `screen` to make sure they don't get interrupted.

## Evaluate reranked results
After reranking is done, let's copy the results from GS to our working directory, where we concatenate all the score files back into one file.
```
gsutil cp ${GS_FOLDER}/scores.txt???-1100000 ${DATA_DIR}/
cat ${DATA_DIR}/scores..txt???-1100000 > ${DATA_DIR}/scores.txt
```

Then we convert the monoT5 output to the required `trec_tool` format.
```
python pygaggle/data/convert_run_from_t5_to_trec_format.py \
    --predictions_path=${DATA_DIR}/score.txt \
    --query_run_ids=${DATA_DIR}/segment_query_doc_ids.tsv \
    --output_path=${DATA_DIR}/run.monot5_${MODEL_NAME}.tsv
```

Now we can evaluate the reranked results using the official MS MARCO evaluation script.
```
python tools/eval/msmarco_eval.py ${DATA_DIR}/qrels.dev.small.tsv ${DATA_DIR}/run.monot5_${MODEL_NAME}.dev.tsv
```

In the case of monoT5-3B, the output should be:

```
#####################
MRR @10: 0.3983799517896949
QueriesRanked: 6980
#####################
```

In the case of monoT5-base, the output should be:

```
#####################
MRR @10: 0.38160657433938283
QueriesRanked: 6980
#####################
```

If you were able to replicate any of these results, please submit a PR adding to the replication log, along with the model(s) you replicated. 
Please mention in your PR if you note any differences.


## Replication Log

